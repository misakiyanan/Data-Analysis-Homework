# **作业:**

## **一、留存**

### **1. 留存的含义:**

留存: 指基准日到APP的用户在之后的n日当天返回APP的人数;     
留存率 = 基准日之后的n天当日返回的用户数 / 基准日的用户数 * 100%  
或者指基准日产生某个行为的用户在之后的第一天，第二天，第三天......第n天的当天再次产生该行为的人数。

### **2.留存的意义:** 

留存代表一个用户愿意再次使用你的产品;  
而一个产品能够被用户再次使用，意味着这个产品是能够满足用户长期需求的，能够让用户产生一定粘性的产品。  
如今互联网产品大多为免费产品，依靠持续的广告转化、用户持续的购买转化、用户持续的会员付费来维持收益。  
那么，怎样才能达到持续的广告转化?持续的购买转化?持续的会员付费?  
广告转化、购买、会员付费，我们都需要用户来完成。  
那么持续的广告转化、购买、会员付费，我们就需要每天都有一定量的用户来完成以上过程，也就是说我们需要我们的产品每天都是有活跃用户的。  

```
活跃用户 = 新增用户 + 留存用户
```

那么从活跃用户的构成，我们就知道怎么维持我们的活跃用户规模: 一方面持续有新用户流入，另一方
面持续有留存用户留存;  
那么为什么留存用户重要?部分有过从业经验的同学应该知道，产品的自然新增用户是比较少的，如果
我们仅靠产品自然用户增长，那么必然我们的活跃用户量级会非常小，虽然能够获得收益，但是这部分收益很可能养不起我们的公司; 但是如果我们想要用购买新用户的方式获得持续的较大规模的用户池，又会产生比较大的成本，这样做容易亏本。所以我们需要新增留存两手抓; 想方设法留住我们新增的用户，让其长期地在我们的产品内产生价值，这样做，比起单做新增，在成本和效果上都是要好的。

### **3.留存的实际应用**

**留存的主要应用场景有两个:** 

一个是产品整体视角的持续留存情况; 也就是我们需要定期分析用户在产品的留存是否维持在一个正常
的范围，有没有突然地涨跌情况，如果有的话问题出在哪里，为什么会出现这样的问题，如何解决留存异常的问题。遇到这类问题首先需要确定的是整体用户的留存异常还是个别群体用户的留存异常，如果是整体用户的留存异常，我们需要分析我们的产品是否发生异常，可以通过用户的行为路径确定异常产品位置;如果是个别用户的留存异常，需要通过用户拆解的方法来定位有异常的用户群体，然后通过这类用户的特征来分析问题原因;

一个是新增用户视角的每批新增用户留存情况。一般出现在计算投放ROI的场景，也就是我们投放一批用户，能够给我们带来多少收益。我们会用这些用户的“整个生命周期能够带来的收入”(LTV)去除以 “投放时的支出”来计算我们的ROI。“整个生命周期能够带来的收入”计算的是引入一批新用户，当用户完 全流失时，这部分用户所带来的所有收入。但是有时我们需要提前预估新用户在整个生命周期能够带来 的收入，我们就需要首先用留存来估计用户的生命周期，然后用估计出的生命周期再去计算用户的整个 生命周期能够带来的收入。

### **4.什么企业不关心留存** 

需要用户产生持续活跃，持续转化，持续消费的企业都关心用户的留存。那么，什么样的企业不关心用户留存?

部分线下企业存在数据难以获取的问题，所以对留存关注比较少，比如线下的商场，除非用户产生购买行为，否则很难探知用户在什么情况下有回流;

部分存在一锤子买卖的企业也不太关心留存，比如说用户在一次买断商品后再也不会产生后续购买或价值转化行为，这类企业也不怎么关注留存。比如PC单机游戏; 比如一次付费的工具类产品(线上线下类工具都存在这个情况);   

## **问题:**

### 1.计算某APP的每日留存数据。用SQL分解留存计算，每步请用视图建立(90分)

如何计算留存:  
dates_a 计算留存的基准日日期。这个基准日可以为任意一天，如果基准日就是今天，也就是没有“之后 的第一天”、“之后的第二天”......等等天的数据，那那几天的留存可以以0、或者空值代替;   
device_v 基准日当日的活跃用户数。不同产品的活跃用户定义不同，在这里，我们将活跃用户定义为在 当天有过打开APP的用户;    
day_1 次日留存率，也就是“基准日之后的第一天”的用户的回访比例。   
day_2 2日留存率，也就是“基准日之后的第二天”的用户的回访比例。  
 day_3 3日留存率，也就是“基准日之后的第三天”的用户的回访比例。   
day_7 7日留存率，也就是“基准日之后的第七天”的用户的回访比例。

也就是说:这个表格，表达的是:每天对应多少活跃用户，每天的活跃用户对应的次留率、2日留率、3 日留率、7日留率是多少。我们能从这个表格中可以看出我们产品的用户活跃程度，以及产品对当日活跃 用户的短期、长期粘性情况。

关键的两个步骤:

* 我们需要将每日活跃的用户数据与之后几日该用户的活跃数据连接起来;这样，我们就能知道这个 用户在之后的哪一天有回访行为;

* 通过SQL的数据列联功能，也就是“聚合函数(case when 条件)"的方式，将满足“基准日之后的第一 天”、“基准日之后的第二天”......的用户数据统计出来，最后再通过基准日的用户数，计算用户留存 率;

#### 1.0.建立用户活跃日期表(5分)

表名: temp_user_act  
字段:  
字段名 字段类型 字段说明   
user_id int 用户id  
dates date 活跃日期 

```sql
create database homework_20210708 charset utf8;
use homework_20210708;

create table temp_user_act (
user_id int,
dates date);
```



#### 1.1.用用户活跃日期表做自连接，连接方式使用左连接，连接字段使用“用户id”字段，保留两表的用户id与两表的日期(20分)

```sql
select *
from temp_user_act t1
left join temp_user_act t2
on t1.user_id = t2.user_id;
```

#### 1.2.筛选出右表日期大于或等于左表日期的内容(20分)

```sql
create view self_joint_view (l_user_id, l_date, r_user_id, r_date) as (
select *
from temp_user_act t1
left join temp_user_act t2
on t1.user_id = t2.user_id
where t2.dates >= t1.dates
);
```

### 1.3.计算以左表日期为基准日的当日用户数，第二日回访用户数，第三日回访用户数，第四日回访用户 数，第八日回访用户数;(20分)

提示:

* datediff(B, A)=1 表示日期B为日期A之后的一天。
* count(distinct case when datediff(B,A)=1 then uid else null end)   
  或者 count(distinct if( datediff(B,A)=1,uid,null))   
  就是求出“日期B为基准日A之后的第一天”的用户数计数

```sql
create view counts_view as (
select l_date, 
count(distinct l_user_id) benchmark_count, 
count(distinct if (datediff(r_date, l_date)=1, l_user_id, NULL)) one_day,
count(distinct if (datediff(r_date, l_date)=2, l_user_id, NULL)) two_day,
count(distinct if (datediff(r_date, l_date)=3, l_user_id, NULL)) three_day,
count(distinct if (datediff(r_date, l_date)=4, l_user_id, NULL)) four_day,
count(distinct if (datediff(r_date, l_date)=8, l_user_id, NULL)) eight_day

from self_joint_view
group by l_date
);

```

```
mysql> select l_date, 
    -> count(distinct l_user_id) benchmark_count, 
    -> count(distinct if (datediff(r_date, l_date)=1, l_user_id, NULL)) one_day,
    -> count(distinct if (datediff(r_date, l_date)=2, l_user_id, NULL)) two_day,
    -> count(distinct if (datediff(r_date, l_date)=3, l_user_id, NULL)) three_day,
    -> count(distinct if (datediff(r_date, l_date)=4, l_user_id, NULL)) four_day,
    -> count(distinct if (datediff(r_date, l_date)=8, l_user_id, NULL)) eight_day
    -> 
    -> from self_joint_view
    -> group by l_date;
+------------+-----------------+---------+---------+-----------+----------+-----------+
| l_date     | benchmark_count | one_day | two_day | three_day | four_day | eight_day |
+------------+-----------------+---------+---------+-----------+----------+-----------+
| 2019-11-18 |             226 |     147 |     146 |       136 |      140 |       121 |
| 2019-11-19 |             222 |     154 |     141 |       137 |      141 |       135 |
| 2019-11-20 |             231 |     146 |     137 |       143 |      150 |       136 |
| 2019-11-21 |             232 |     144 |     146 |       151 |      152 |       141 |
| 2019-11-22 |             226 |     157 |     145 |       148 |      136 |       147 |
| 2019-11-23 |             241 |     161 |     160 |       141 |      147 |       153 |
| 2019-11-24 |             236 |     163 |     151 |       157 |      154 |       151 |
| 2019-11-25 |             243 |     157 |     154 |       161 |      154 |       143 |
| 2019-11-26 |             221 |     150 |     149 |       144 |      141 |       137 |
| 2019-11-27 |             232 |     164 |     154 |       152 |      156 |       145 |
| 2019-11-28 |             240 |     163 |     152 |       154 |      142 |       145 |
| 2019-11-29 |             243 |     169 |     164 |       155 |      147 |       148 |
| 2019-11-30 |             244 |     155 |     166 |       158 |      161 |       155 |
| 2019-12-01 |             245 |     162 |     148 |       162 |      165 |       155 |
| 2019-12-02 |             241 |     162 |     161 |       162 |      161 |       158 |
| 2019-12-03 |             235 |     164 |     158 |       158 |      153 |       159 |
| 2019-12-04 |             247 |     168 |     169 |       163 |      163 |       205 |
| 2019-12-05 |             242 |     165 |     155 |       159 |      160 |       149 |
| 2019-12-06 |             250 |     163 |     161 |       161 |      162 |       143 |
| 2019-12-07 |             241 |     167 |     165 |       168 |      173 |       155 |
| 2019-12-08 |             247 |     171 |     173 |       181 |      201 |       155 |
| 2019-12-09 |             251 |     173 |     177 |       202 |      155 |       156 |
| 2019-12-10 |             254 |     190 |     207 |       166 |      147 |       145 |
| 2019-12-11 |             272 |     224 |     174 |       166 |      168 |         0 |
| 2019-12-12 |             324 |     203 |     193 |       195 |      187 |         0 |
| 2019-12-13 |             245 |     155 |     162 |       146 |      160 |         0 |
| 2019-12-14 |             235 |     161 |     152 |       151 |      140 |         0 |
| 2019-12-15 |             243 |     168 |     158 |       140 |        0 |         0 |
| 2019-12-16 |             245 |     166 |     143 |         0 |        0 |         0 |
| 2019-12-17 |             238 |     154 |       0 |         0 |        0 |         0 |
| 2019-12-18 |             224 |       0 |       0 |         0 |        0 |         0 |
+------------+-----------------+---------+---------+-----------+----------+-----------+
31 rows in set (0.12 sec)

```

### 1.4.利用上述数据计算出每日的当日用户数以及次日留存率，二日留存率，三日留存率，7日留存率(率需 要使用百分比表示结果);(20分)

(在下一部分的案例中，我们会给大家介绍一个一次性完成的留存计 算方法，给大家加深留存计算的印象。)

```sql
select l_date,
concat(one_day / benchmark_count * 100, '%') as oneday_per,
concat(two_day / benchmark_count * 100, '%') as twoday_per,
concat(three_day / benchmark_count * 100, '%') as threeday_per,
concat(four_day / benchmark_count * 100, '%') as fourday_per,
concat(eight_day / benchmark_count * 100, '%') as eightday_per
from counts_view;
```

```
mysql> select l_date,
    -> concat(one_day / benchmark_count * 100, '%') as oneday_per,
    -> concat(two_day / benchmark_count * 100, '%') as twoday_per,
    -> concat(three_day / benchmark_count * 100, '%') as threeday_per,
    -> concat(four_day / benchmark_count * 100, '%') as fourday_per,
    -> concat(eight_day / benchmark_count * 100, '%') as eightday_per
    -> from counts_view;
+------------+------------+------------+--------------+-------------+--------------+
| l_date     | oneday_per | twoday_per | threeday_per | fourday_per | eightday_per |
+------------+------------+------------+--------------+-------------+--------------+
| 2019-11-18 | 65.0442%   | 64.6018%   | 60.1770%     | 61.9469%    | 53.5398%     |
| 2019-11-19 | 69.3694%   | 63.5135%   | 61.7117%     | 63.5135%    | 60.8108%     |
| 2019-11-20 | 63.2035%   | 59.3074%   | 61.9048%     | 64.9351%    | 58.8745%     |
| 2019-11-21 | 62.0690%   | 62.9310%   | 65.0862%     | 65.5172%    | 60.7759%     |
| 2019-11-22 | 69.4690%   | 64.1593%   | 65.4867%     | 60.1770%    | 65.0442%     |
| 2019-11-23 | 66.8050%   | 66.3900%   | 58.5062%     | 60.9959%    | 63.4855%     |
| 2019-11-24 | 69.0678%   | 63.9831%   | 66.5254%     | 65.2542%    | 63.9831%     |
| 2019-11-25 | 64.6091%   | 63.3745%   | 66.2551%     | 63.3745%    | 58.8477%     |
| 2019-11-26 | 67.8733%   | 67.4208%   | 65.1584%     | 63.8009%    | 61.9910%     |
| 2019-11-27 | 70.6897%   | 66.3793%   | 65.5172%     | 67.2414%    | 62.5000%     |
| 2019-11-28 | 67.9167%   | 63.3333%   | 64.1667%     | 59.1667%    | 60.4167%     |
| 2019-11-29 | 69.5473%   | 67.4897%   | 63.7860%     | 60.4938%    | 60.9053%     |
| 2019-11-30 | 63.5246%   | 68.0328%   | 64.7541%     | 65.9836%    | 63.5246%     |
| 2019-12-01 | 66.1224%   | 60.4082%   | 66.1224%     | 67.3469%    | 63.2653%     |
| 2019-12-02 | 67.2199%   | 66.8050%   | 67.2199%     | 66.8050%    | 65.5602%     |
| 2019-12-03 | 69.7872%   | 67.2340%   | 67.2340%     | 65.1064%    | 67.6596%     |
| 2019-12-04 | 68.0162%   | 68.4211%   | 65.9919%     | 65.9919%    | 82.9960%     |
| 2019-12-05 | 68.1818%   | 64.0496%   | 65.7025%     | 66.1157%    | 61.5702%     |
| 2019-12-06 | 65.2000%   | 64.4000%   | 64.4000%     | 64.8000%    | 57.2000%     |
| 2019-12-07 | 69.2946%   | 68.4647%   | 69.7095%     | 71.7842%    | 64.3154%     |
| 2019-12-08 | 69.2308%   | 70.0405%   | 73.2794%     | 81.3765%    | 62.7530%     |
| 2019-12-09 | 68.9243%   | 70.5179%   | 80.4781%     | 61.7530%    | 62.1514%     |
| 2019-12-10 | 74.8031%   | 81.4961%   | 65.3543%     | 57.8740%    | 57.0866%     |
| 2019-12-11 | 82.3529%   | 63.9706%   | 61.0294%     | 61.7647%    | 0.0000%      |
| 2019-12-12 | 62.6543%   | 59.5679%   | 60.1852%     | 57.7160%    | 0.0000%      |
| 2019-12-13 | 63.2653%   | 66.1224%   | 59.5918%     | 65.3061%    | 0.0000%      |
| 2019-12-14 | 68.5106%   | 64.6809%   | 64.2553%     | 59.5745%    | 0.0000%      |
| 2019-12-15 | 69.1358%   | 65.0206%   | 57.6132%     | 0.0000%     | 0.0000%      |
| 2019-12-16 | 67.7551%   | 58.3673%   | 0.0000%      | 0.0000%     | 0.0000%      |
| 2019-12-17 | 64.7059%   | 0.0000%    | 0.0000%      | 0.0000%     | 0.0000%      |
| 2019-12-18 | 0.0000%    | 0.0000%    | 0.0000%      | 0.0000%     | 0.0000%      |
+------------+------------+------------+--------------+-------------+--------------+
31 rows in set (0.12 sec)
```

### 1.5.求出每日的次留与次留的周环比(5分)
 注: 周环比:周环比适用于以周为周期变动的数据，用于观察每天的数据较上周同一天的数据变化。

```
现有互联网数据多以周为周期变动，一周内每一天的数据都有其特点:

如工作类APP，周一会是每一周的高峰，然后向周五逐渐递减，周五到周六又会有一个锐减的过程，周日与周六趋于平缓;
如娱乐类APP，周一会较周末锐减，然后向周四逐渐递减，周五会有一个跃升的过程，周六会再次 跃升，周日会较周六下降;
```

一般遇到这样的变动趋势，如果单纯的分析每一天较上一天的变动情况，就容易分析出错误结论。所
以，在我们分析带有周期类数据时，一定要先将周期提炼出来，使用周期的环比数据做分析，会得出更
有价值，更加准确的结论。

* 计算方式:
  假如今日为周一，那么周环比=(本周一数据 - 上周一数据) / 上周一数据 * 100%   
  没有上周对应日期的数据的部分可以为空;

```sql
select 
l_date as benchmark_date,
two_day as twoday_count,
lag(two_day, 7) over(order by current_date asc) twoday_count_last_week,
concat((two_day-lag(two_day, 7) over(order by current_date asc)) / lag(two_day, 7) over(order by current_date asc) * 100, '%') as chain_ratio
from counts_view;
```

```
mysql> select 
    -> l_date as benchmark_date,
    -> two_day as twoday_count,
    -> lag(two_day, 7) over(order by current_date asc) twoday_count_last_week,
    -> concat((two_day-lag(two_day, 7) over(order by current_date asc)) / lag(two_day, 7) over(order by current_date asc) * 100, '%') as chain_ratio
    -> from counts_view;
+----------------+--------------+------------------------+-------------+
| benchmark_date | twoday_count | twoday_count_last_week | chain_ratio |
+----------------+--------------+------------------------+-------------+
| 2019-11-18     |          146 |                   NULL | NULL        |
| 2019-11-19     |          141 |                   NULL | NULL        |
| 2019-11-20     |          137 |                   NULL | NULL        |
| 2019-11-21     |          146 |                   NULL | NULL        |
| 2019-11-22     |          145 |                   NULL | NULL        |
| 2019-11-23     |          160 |                   NULL | NULL        |
| 2019-11-24     |          151 |                   NULL | NULL        |
| 2019-11-25     |          154 |                    146 | 5.4795%     |
| 2019-11-26     |          149 |                    141 | 5.6738%     |
| 2019-11-27     |          154 |                    137 | 12.4088%    |
| 2019-11-28     |          152 |                    146 | 4.1096%     |
| 2019-11-29     |          164 |                    145 | 13.1034%    |
| 2019-11-30     |          166 |                    160 | 3.7500%     |
| 2019-12-01     |          148 |                    151 | -1.9868%    |
| 2019-12-02     |          161 |                    154 | 4.5455%     |
| 2019-12-03     |          158 |                    149 | 6.0403%     |
| 2019-12-04     |          169 |                    154 | 9.7403%     |
| 2019-12-05     |          155 |                    152 | 1.9737%     |
| 2019-12-06     |          161 |                    164 | -1.8293%    |
| 2019-12-07     |          165 |                    166 | -0.6024%    |
| 2019-12-08     |          173 |                    148 | 16.8919%    |
| 2019-12-09     |          177 |                    161 | 9.9379%     |
| 2019-12-10     |          207 |                    158 | 31.0127%    |
| 2019-12-11     |          174 |                    169 | 2.9586%     |
| 2019-12-12     |          193 |                    155 | 24.5161%    |
| 2019-12-13     |          162 |                    161 | 0.6211%     |
| 2019-12-14     |          152 |                    165 | -7.8788%    |
| 2019-12-15     |          158 |                    173 | -8.6705%    |
| 2019-12-16     |          143 |                    177 | -19.2090%   |
| 2019-12-17     |            0 |                    207 | -100.0000%  |
| 2019-12-18     |            0 |                    174 | -100.0000%  |
+----------------+--------------+------------------------+-------------+
31 rows in set (0.12 sec)
```



## 二 作者活跃行为统计(10分)

如今的大多数内容APP都是以平台模式运营的。

什么叫做平台模式?

比如我们现在能接触到的大多数内容产品“抖音”、“Bilibili”、“微博”、“今日头条”等等。我们使用到的这个 APP，是由APP的制作公司提供的，也就是这个APP会有几个主要的页面，每个页面怎么引导用户进入内 容页面，播放内容的方式有哪些、内容间切换会用什么方式等等，都是我们的APP制作公司决定的;而 我们在这个APP里看到的大多数内容，都是由内容作者提供的。

为了能够让我们的平台能够有持续的内容产出，我们就需要分析作者的活跃行为。主要为我们的作者运营提供有力帮助。一方面让我们找出活跃作者，增加政策扶持; 一方面让我们找到快要流失的作者，及时召回这部分作者、挽回损失。

**某短视频公司有作者发布视频统计表如下:  **  
表名: temp_author_act   
字段:   
字段名 字段类型 字段说明  
dates date 发布日期   
author_id varchar(5) 作者id

#### 2.1.请求用SQL出作者的最近三个月内的最大断更天数、平均断更天数和最大持续更新天数(5分)

名词解释

**持续更新天数:** 如果一个作者在某几天中每一天都有更新，那么这段时间的天数称为这个作者的持续更新天数。   
**断更天数:** 如果一个作者两次更新中间隔了几天没有更新，那么这几天的天数称为这个作者的断更天数。

```sql
use homework_20210708;

create table author_act (
dates date,
author_id varchar(5));
```

```sql
create view view_1 as(
  with stats_t as(
    select 
    author_id, 
    dates,
    lag(dates, 1) over(partition by author_id order by dates) as last_post,
    datediff(dates, lag(dates, 1) over(partition by author_id order by dates)) as date_diff,
    datediff(dates, lag(dates, 1) over(partition by author_id order by dates))-1 as duangengtianshu
    from author_act
  )

select 
author_id, 
max(duangengtianshu) as 最大断更, 
avg(duangengtianshu) as 平均断更
from stats_t
group by author_id);
```

```sql
create view view_2 as (
  with stats_t as(
    select 
    author_id, 
    dates,
    lag(dates, 1) over(partition by author_id order by dates) as last_post,
    datediff(dates, lag(dates, 1) over(partition by author_id order by dates)) as date_diff,
    datediff(dates, lag(dates, 1) over(partition by author_id order by dates))-1 as duangengtianshu
    from author_act
  )
  
  select b.author_id,
  max(b.lianxugengxintianshu) as 最大连续更新天数
  from (
    select 
    author_id,
    a.sum_,
    count(*) as lianxugengxintianshu
    from (
      select 
      author_id,
      duangengtianshu,
      row_number() over(partition by author_id order by dates) as rownumber_,
      sum(duangengtianshu) over(partition by author_id order by dates) as sum_
      from stats_t)a
    group by a.author_id, a.sum_)b
  
  group by b.author_id);

```

```sql
select view_1.author_id, 最大断更, 平均断更, 最大连续更新天数    
from view_1 
join view_2 
on view_1.author_id = view_2.author_id;
```

```
mysql> select view_1.author_id, 最大断更, 平均断更, 最大连续更新天数    
    -> from view_1 join view_2 on view_1.author_id = view_2.author_id;
+-----------+--------------+--------------+--------------------------+
| author_id | 最大断更       | 平均断更      |    最大连续更新天数         |
+-----------+--------------+--------------+--------------------------+
| a001      |            6 |       0.6667 |                        6 |
| b001      |           11 |       5.9231 |                        2 |
| c001      |            7 |       0.4286 |                       15 |
+-----------+--------------+--------------+--------------------------+
3 rows in set (0.01 sec)
```



#### 2.2.运营人员需要对作者做电话访问，需要你用SQL求出每位作者在最大断更天数时对应的日期范围。 用于访问该日期内的断更原因。(5分)



